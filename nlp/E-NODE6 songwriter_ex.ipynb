{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* glob 모듈 -  파일을 읽어오는 작업 용이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['Ghost in the mirror', \"I knew your face once, but now it's unclear\", \"And I can't feel my body now\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!\n",
    "\n",
    "preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다.\n",
    "\n",
    "추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다. 너무 긴 문장은 노래가사 작사하기에 어울리지 않을수도 있겠죠.\n",
    "그래서 이번에는 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외하기를 권합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ghost in the mirror', \"I knew your face once, but now it's unclear\", \"And I can't feel my body now\", \"I'm separate from here and now A drug and a dream\", 'We lost connection, oh come back to me', 'So I can feel alive again', \"Soul and body try to mend It's pulling me apart, this time\", 'Everything is never ending', 'Slipped into a peril that', \"I'll never understand\"]\n"
     ]
    }
   ],
   "source": [
    "print(raw_corpus[:10])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 너무 긴 문장은 쉼표기준으로도 분리할 필요있어보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제 - (대소문자 통일)\n",
    "\n",
    "    # 특문 >> (공백)특문(공백) 안됨 -- r'\\1'\n",
    "    # 특문 >> 공백\n",
    "    #()는 코러스인데 삭제하는게 낫지않을까\n",
    "    sentence = re.sub(r\"[(a-zA-Z,.)],\",\" \", sentence)\n",
    "    sentence = re.sub(r\"[.,]\", r\" \", sentence)        \n",
    "        \n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?'.!,¿]+\", \" \", sentence)  # 영어대소문자, 특문아닌 것(^)은 스페이스 1개로 치환\n",
    "    #it's 같은거 살리기\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> ghost in the mirror <end>',\n",
       " \"<start> i knew your face onc but now it's unclear <end>\",\n",
       " \"<start> and i can't feel my body now <end>\",\n",
       " \"<start> i'm separate from here and now a drug and a dream <end>\",\n",
       " '<start> we lost connectio oh come back to me <end>',\n",
       " '<start> so i can feel alive again <end>',\n",
       " \"<start> soul and body try to mend it's pulling me apar this time <end>\",\n",
       " '<start> everything is never ending <end>',\n",
       " '<start> slipped into a peril that <end>',\n",
       " \"<start> i'll never understand <end>\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if len(sentence.split()) >= 15: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> ghost in the mirror <end>',\n",
       " \"<start> i knew your face onc but now it's unclear <end>\",\n",
       " \"<start> and i can't feel my body now <end>\",\n",
       " \"<start> i'm separate from here and now a drug and a dream <end>\",\n",
       " '<start> we lost connectio oh come back to me <end>',\n",
       " '<start> so i can feel alive again <end>',\n",
       " \"<start> soul and body try to mend it's pulling me apar this time <end>\",\n",
       " '<start> everything is never ending <end>',\n",
       " '<start> slipped into a peril that <end>',\n",
       " \"<start> i'll never understand <end>\",\n",
       " '<start> this feeling always gets away <end>',\n",
       " '<start> wishing i could hold on longer <end>',\n",
       " \"<start> it doesn't have to feel so strange <end>\",\n",
       " \"<start> i knew your face onc but now it's unclear <end>\",\n",
       " \"<start> and i can't feel my body now <end>\",\n",
       " \"<start> i'm separate from here and now it's pulling me apar this time <end>\",\n",
       " '<start> everything is never ending <end>',\n",
       " '<start> slipped into a peril that <end>',\n",
       " \"<start> i'll never understand <end>\",\n",
       " '<start> this feeling always gets away <end>',\n",
       " '<start> wishing i could hold on longer <end>',\n",
       " \"<start> it doesn't have to feel so strange <end>\",\n",
       " '<start> baby baby baby baby baby yeah <end>',\n",
       " '<start> used to love yo used to love you <end>',\n",
       " '<start> used to love yo used to love you <end>',\n",
       " '<start> used to love yo used to love you <end>',\n",
       " '<start> used to love yo used to love you verse rihanna <end>',\n",
       " '<start> every nigh every day <end>',\n",
       " \"<start> o i be thinking yeaah 'bout the way you touch me <end>\",\n",
       " \"<start> 'bout the way you kiss m when i'm sleeping chorus rihanna chris brown <end>\",\n",
       " '<start> used to love yo used to love you <end>',\n",
       " '<start> used to love yo used to love you <end>',\n",
       " '<start> used to love yo used to love you bab ahhh <end>',\n",
       " '<start> used to love yo used to love you verse chris brown <end>',\n",
       " \"<start> by the wa what's his name <end>\",\n",
       " '<start> i want you eh yeahh <end>',\n",
       " '<start> i belong with yo you belong with me <end>',\n",
       " '<start> what were you thinking chorus rihanna chris brown <end>',\n",
       " '<start> used to love yo used to love you <end>',\n",
       " '<start> used to love yo used to love you <end>',\n",
       " '<start> used to love yo used to love you bab ahhh <end>',\n",
       " '<start> used to love yo used to love you bridge rihanna chris brown <end>',\n",
       " \"<start> can't nobody <end>\",\n",
       " '<start> love me like you nobody baby <end>',\n",
       " \"<start> can't nobody ohh <end>\",\n",
       " '<start> hold me like you nobody <end>',\n",
       " \"<start> can't nobod baby can't nobody love me <end>\",\n",
       " '<start> kiss me like you <end>',\n",
       " \"<start> can't nobody baby ain't nobody love me now <end>\",\n",
       " \"<start> ain't nobod baby no <end>\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 토크나이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. 단어장의 크기는 12,000 이상으로 설정하세요! 총 데이터의 20%를 평가 데이터셋으로 사용해 주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 1373   12 ...    0    0    0]\n",
      " [   2    5  385 ...    0    0    0]\n",
      " [   2    7    5 ...    0    0    0]\n",
      " ...\n",
      " [   2   72   38 ...    3    0    0]\n",
      " [   2  103  754 ...    0    0    0]\n",
      " [   2   10  837 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f257aae87d0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  \n",
    "        filters=' ',    \n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델의 최종 출력 텐서 shape 어떻게 확인? (1760000개 문장이 최대 15줄, 5000개 단어사용했는데)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166092, 15)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 1373   12    4  864    3    0    0    0    0]\n",
      " [   2    5  385   17  218 3527   27   48   50 8041]\n",
      " [   2    7    5   79  101   10  243   48    3    0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10]) # 3행,10열까지만"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 단어사전 형태확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : the\n",
      "5 : i\n",
      "6 : you\n",
      "7 : and\n",
      "8 : a\n",
      "9 : to\n",
      "10 : my\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2로 시작해서 3으로 끝나거나, 길어서 안끝나거나.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2 1373   12    4  864    3    0    0    0    0    0    0    0    0]\n",
      "[1373   12    4  864    3    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터셋 객체 생성  \n",
    " 실제문장과 다르게 시작과 끝만 주고 나머지는 모델이 학습하면서 채워넣게 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((166092, 14), (166092, 14))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_input.shape, tgt_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((1024, 14), (1024, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 1024\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # 사전 + 포함되지 않은 0:<pad>를 포함하여 5001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source Train: (124960, 14)\n",
    "# Target Train: (124960, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (132873, 14)\n",
      "Target Train: (132873, 14)\n",
      "(33219, 14)\n",
      "(33219, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "print(enc_val.shape)\n",
    "print(dec_val.shape)\n",
    "# 더 어떻게 줄이는지 모르겠음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1024, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 3.77360877e-04,  1.32223759e-05,  4.77482463e-05, ...,\n",
       "         -6.39169375e-05,  3.30271178e-05, -3.16581660e-04],\n",
       "        [ 5.19942492e-04, -1.48055507e-04, -6.53131647e-05, ...,\n",
       "         -2.70952096e-05,  5.41001245e-05, -4.46536491e-04],\n",
       "        [ 6.15278492e-04, -1.29224733e-04, -1.75283232e-04, ...,\n",
       "          5.75746162e-05, -1.76370420e-04, -3.69369634e-04],\n",
       "        ...,\n",
       "        [ 8.22272385e-04,  2.55492312e-04, -1.77251459e-05, ...,\n",
       "          2.17828993e-03, -1.71253399e-03, -8.03825154e-04],\n",
       "        [ 7.28114042e-04,  3.21790925e-04, -1.12756104e-04, ...,\n",
       "          2.33058515e-03, -1.99354673e-03, -9.96105606e-04],\n",
       "        [ 6.29782618e-04,  3.69876856e-04, -2.18209170e-04, ...,\n",
       "          2.45313067e-03, -2.26069218e-03, -1.17605389e-03]],\n",
       "\n",
       "       [[ 3.77360877e-04,  1.32223759e-05,  4.77482463e-05, ...,\n",
       "         -6.39169375e-05,  3.30271178e-05, -3.16581660e-04],\n",
       "        [ 4.80735122e-04, -2.89942283e-04,  8.02947179e-05, ...,\n",
       "         -5.47961099e-05,  1.00340585e-04, -5.59059437e-04],\n",
       "        [ 4.95793065e-04, -2.48911267e-04,  1.11035733e-04, ...,\n",
       "          1.56479728e-04,  2.01768620e-04, -4.72023094e-04],\n",
       "        ...,\n",
       "        [ 7.43147742e-04,  4.05576429e-04,  1.58882540e-05, ...,\n",
       "          1.39898888e-03, -1.71577802e-03, -3.97135358e-04],\n",
       "        [ 7.13390065e-04,  4.08685795e-04, -1.03669220e-06, ...,\n",
       "          1.70553580e-03, -2.05258164e-03, -6.11355703e-04],\n",
       "        [ 6.55437412e-04,  4.05371160e-04, -4.24050777e-05, ...,\n",
       "          1.96554721e-03, -2.35773879e-03, -8.20078014e-04]],\n",
       "\n",
       "       [[ 3.77360877e-04,  1.32223759e-05,  4.77482463e-05, ...,\n",
       "         -6.39169375e-05,  3.30271178e-05, -3.16581660e-04],\n",
       "        [ 5.08940022e-04,  7.44696517e-05,  3.87147767e-04, ...,\n",
       "         -2.65914074e-04, -6.58575154e-05, -5.18507324e-04],\n",
       "        [ 4.81638650e-04, -3.22843225e-05,  4.69343271e-04, ...,\n",
       "         -1.83684038e-04, -9.67504675e-05, -3.41751467e-04],\n",
       "        ...,\n",
       "        [ 3.68633133e-04, -7.03774494e-05,  5.74070960e-04, ...,\n",
       "          2.11837795e-03, -2.00489140e-03, -7.26958679e-04],\n",
       "        [ 3.14628065e-04,  6.81053143e-06,  4.03383223e-04, ...,\n",
       "          2.32765405e-03, -2.27106712e-03, -9.02044354e-04],\n",
       "        [ 2.55097460e-04,  7.20719909e-05,  2.31056081e-04, ...,\n",
       "          2.49192258e-03, -2.51792814e-03, -1.06976205e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 3.77360877e-04,  1.32223759e-05,  4.77482463e-05, ...,\n",
       "         -6.39169375e-05,  3.30271178e-05, -3.16581660e-04],\n",
       "        [ 5.15960041e-04,  3.31899173e-05,  2.42055554e-04, ...,\n",
       "         -2.38799708e-04,  2.10590606e-05, -5.32222970e-04],\n",
       "        [ 4.97931556e-04, -2.14027561e-04,  2.88983400e-04, ...,\n",
       "         -3.06957081e-04,  9.80813275e-05, -7.27802864e-04],\n",
       "        ...,\n",
       "        [ 7.41706928e-04,  3.98356569e-05,  1.65134554e-06, ...,\n",
       "          2.10023066e-03, -1.30560633e-03, -9.35225922e-04],\n",
       "        [ 6.71242073e-04,  1.07924679e-04, -1.10497211e-04, ...,\n",
       "          2.30358727e-03, -1.66581559e-03, -1.08269881e-03],\n",
       "        [ 5.89073170e-04,  1.58003691e-04, -2.23359311e-04, ...,\n",
       "          2.46476359e-03, -2.00226647e-03, -1.22447580e-03]],\n",
       "\n",
       "       [[ 3.77360877e-04,  1.32223759e-05,  4.77482463e-05, ...,\n",
       "         -6.39169375e-05,  3.30271178e-05, -3.16581660e-04],\n",
       "        [ 5.58293716e-04,  1.78554488e-04,  1.97250833e-04, ...,\n",
       "         -9.56518052e-05, -9.62142658e-05, -2.21252005e-04],\n",
       "        [ 5.82370034e-04,  2.32233127e-04,  5.33237835e-05, ...,\n",
       "         -9.42212355e-05, -1.43999860e-04, -2.22369243e-04],\n",
       "        ...,\n",
       "        [ 3.81792925e-04,  6.44300599e-04,  9.07008052e-06, ...,\n",
       "          2.26907758e-03, -2.31210678e-03, -1.20536785e-03],\n",
       "        [ 3.34454438e-04,  6.19565893e-04, -1.12154470e-04, ...,\n",
       "          2.41899956e-03, -2.56074290e-03, -1.37743144e-03],\n",
       "        [ 2.85950606e-04,  5.86224138e-04, -2.40975074e-04, ...,\n",
       "          2.53650383e-03, -2.78530992e-03, -1.53514266e-03]],\n",
       "\n",
       "       [[ 3.77360877e-04,  1.32223759e-05,  4.77482463e-05, ...,\n",
       "         -6.39169375e-05,  3.30271178e-05, -3.16581660e-04],\n",
       "        [ 7.95724045e-04, -4.19240532e-05, -2.62937159e-04, ...,\n",
       "         -3.09529678e-05, -5.56756713e-05, -5.85478730e-04],\n",
       "        [ 1.00698904e-03,  2.51882429e-05, -2.36765292e-04, ...,\n",
       "         -2.77925748e-04, -1.15329145e-04, -5.00788155e-04],\n",
       "        ...,\n",
       "        [ 1.08368497e-03,  1.11788817e-04,  3.40946164e-04, ...,\n",
       "          1.34098533e-04, -4.94585140e-04, -6.56936492e-04],\n",
       "        [ 1.09478168e-03,  6.63373357e-05,  3.24292021e-04, ...,\n",
       "          6.22183259e-04, -7.51480286e-04, -7.65523699e-04],\n",
       "        [ 1.07623811e-03,  6.37508929e-05,  2.80394335e-04, ...,\n",
       "          1.07681006e-03, -1.03886216e-03, -8.94313620e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 최종 출력 텐서 shape?? 256,14,5000 어떻게 확인??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1024, 14])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  1574912   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  2099200   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  6156513   \n",
      "=================================================================\n",
      "Total params: 12,902,881\n",
      "Trainable params: 12,902,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4153/4153 [==============================] - 127s 31ms/step - loss: 3.2161 - val_loss: 2.9886\n",
      "Epoch 2/10\n",
      "4153/4153 [==============================] - 126s 30ms/step - loss: 2.8616 - val_loss: 2.8380\n",
      "Epoch 3/10\n",
      "4153/4153 [==============================] - 122s 29ms/step - loss: 2.6786 - val_loss: 2.7571\n",
      "Epoch 4/10\n",
      "4153/4153 [==============================] - 126s 30ms/step - loss: 2.5293 - val_loss: 2.7179\n",
      "Epoch 5/10\n",
      "4153/4153 [==============================] - 128s 31ms/step - loss: 2.4018 - val_loss: 2.6962\n",
      "Epoch 6/10\n",
      "4153/4153 [==============================] - 129s 31ms/step - loss: 2.2913 - val_loss: 2.6842\n",
      "Epoch 7/10\n",
      "4153/4153 [==============================] - 129s 31ms/step - loss: 2.1923 - val_loss: 2.6790\n",
      "Epoch 8/10\n",
      "4153/4153 [==============================] - 130s 31ms/step - loss: 2.1025 - val_loss: 2.6870\n",
      "Epoch 9/10\n",
      "4153/4153 [==============================] - 132s 32ms/step - loss: 2.0214 - val_loss: 2.6867\n",
      "Epoch 10/10\n",
      "4153/4153 [==============================] - 131s 31ms/step - loss: 1.9465 - val_loss: 2.6883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f249840a550>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(enc_train, dec_train, epochs=10, validation_data=(enc_val,dec_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss 0.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이부분은 잘 몰라서 노드의 코드 가져옴 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence]) \n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> long time i need you to be <end> '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> long time\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> army can be your friend <end> '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> army can\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unk??\n",
    "# loss 2.2는 되는데 val_loss는 왜 2.2가 안될까\n",
    "# 문장이 끊기지 않고 이어져야하는 부분에서 끊겨버림\n",
    "# 모델3개를 순서대로 돌려서 파라미터가 약 1300만개나 됨, loss가 불만족스운 경우 재설정한 모델의 실행에 시간이 많이 걸림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
